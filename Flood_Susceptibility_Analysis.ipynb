{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 洪水易感性分析\n",
    "\n",
    "本notebook用于分析研究区域的洪水易感性，使用机器学习方法基于地形因子和历史洪水数据进行评估。\n",
    "\n",
    "## 分析流程\n",
    "1. 数据准备和预处理\n",
    "2. 特征工程\n",
    "3. 模型训练和评估\n",
    "4. 生成易感性地图\n",
    "5. 结果可视化和分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 检查 PyTorch 是否安装成功\n",
    "print(\"PyTorch 版本:\", torch.__version__)\n",
    "\n",
    "# 检查 GPU 是否可用\n",
    "print(\"GPU 可用:\", torch.cuda.is_available())\n",
    "\n",
    "# 检查 CUDA 版本\n",
    "print(\"CUDA 版本:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 基础数据处理库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# 空间数据处理库\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "\n",
    "# 可视化库\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 检查GPU是否可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据路径\n",
    "flood_points_file = \"Sample/flood_points_utm49n.shp\"\n",
    "non_flood_points_file = \"Sample/non_flood_points_utm49n.shp\"\n",
    "\n",
    "# 定义栅格数据路径\n",
    "raster_files = {\n",
    "    'dem': 'DEM/DEM_fill_UTM49N.tif',\n",
    "    'slope': '1RichDem_code/slope_degrees.tif',\n",
    "    'aspect': '1RichDem_code/aspect.tif',\n",
    "    'curvature': '1RichDem_code/curvature.tif',\n",
    "    'profile_curvature': '1RichDem_code/profile_curvature.tif',\n",
    "    'plan_curvature': '1RichDem_code/planform_curvature.tif',\n",
    "    'twi': '1RichDem_code/twi.tif',\n",
    "    'spi': '1RichDem_code/spi.tif',\n",
    "    'tpi': 'TPI/TopographicPositionIndex.tif',\n",
    "    'tri': 'TRI/Terrain_Ruggedness_Index.tif',\n",
    "    'ap': 'AP/AnnualPrecipitation.tif',\n",
    "    'amdp': 'AMDP/gz_30m_amp(mean_2013-2022).tif',\n",
    "    'ahrf': 'AHRF/2013-2022_heavy_rain_day(mean)_utm_gz.tif',\n",
    "    'clcd': 'CLCD/CLCD_2.10.tif',\n",
    "    'ndvi': 'NDVI/2022_Guangzhou_NDVI_UTM49N.tif',\n",
    "    'd2r': 'D2R/Dist2River_gz.tif',\n",
    "    'dd': 'DD/DrainageDensity.tif'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raster_data(raster_path):\n",
    "    \"\"\"读取栅格数据并处理NODATA值\"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        array = src.read(1, masked=False)\n",
    "        nodata = src.nodata\n",
    "        transform = src.transform\n",
    "        \n",
    "        # 处理NODATA值\n",
    "        if nodata is not None:\n",
    "            array = np.where(array == nodata, np.nan, array)\n",
    "            \n",
    "        return array, transform\n",
    "\n",
    "def extract_values_at_points_gpu(array, transform, points_gdf, device='cuda'):\n",
    "    \"\"\"使用GPU加速栅格值提取\"\"\"\n",
    "    try:\n",
    "        # 转换为float32以减少内存使用\n",
    "        array = array.astype(np.float32)\n",
    "        raster_data = torch.from_numpy(array).to(device)\n",
    "        \n",
    "        coords = [(point.x, point.y) for point in points_gdf.geometry]\n",
    "        rows, cols = zip(*[rasterio.transform.rowcol(transform, x, y) for x, y in coords])\n",
    "        \n",
    "        rows = torch.LongTensor(rows).to(device)\n",
    "        cols = torch.LongTensor(cols).to(device)\n",
    "        values = raster_data[rows, cols].cpu().numpy()\n",
    "        \n",
    "        # 清理GPU内存\n",
    "        del raster_data\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return values\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_values_at_points_gpu: {e}\")\n",
    "        return None\n",
    "\n",
    "# 读取点数据\n",
    "try:\n",
    "    flood_points = gpd.read_file(flood_points_file)\n",
    "    non_flood_points = gpd.read_file(non_flood_points_file)\n",
    "\n",
    "    # 添加标签\n",
    "    flood_points['label'] = 1\n",
    "    non_flood_points['label'] = 0\n",
    "\n",
    "    # 合并数据集\n",
    "    all_points = pd.concat([flood_points, non_flood_points])\n",
    "    print(\"点数据读取成功\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading point data: {e}\")\n",
    "\n",
    "# 提取栅格值\n",
    "try:\n",
    "    for factor, path in raster_files.items():\n",
    "        print(f\"处理 {factor}...\")\n",
    "        array, transform = read_raster_data(path)\n",
    "        values = extract_values_at_points_gpu(array, transform, all_points, device)\n",
    "        if values is not None:\n",
    "            all_points[factor] = values\n",
    "        # 清理内存\n",
    "        del array\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"栅格值提取完成\")\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting raster values: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据探索性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义特征列表\n",
    "features = ['dem', 'slope', 'aspect', 'curvature', 'profile_curvature', \n",
    "           'plan_curvature', 'twi', 'spi', 'tpi', 'tri', 'ap', 'amdp', 'ahrf', 'clcd', 'ndvi', 'd2r', 'dd']\n",
    "\n",
    "# 准备数据\n",
    "X = all_points[features].values.astype(np.float32)\n",
    "\n",
    "# 处理无效值\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X = torch.FloatTensor(X)\n",
    "y = torch.LongTensor(all_points['label'].values)\n",
    "\n",
    "# 数据标准化\n",
    "eps = 1e-8  # 添加小的常数避免除零\n",
    "mean = X.mean(dim=0)\n",
    "std = X.std(dim=0) + eps\n",
    "X = (X - mean) / std\n",
    "\n",
    "# 显示数据集基本信息\n",
    "print(\"数据集形状:\", all_points.shape)\n",
    "print(\"\\n特征列表:\")\n",
    "print(all_points.columns.tolist())\n",
    "print(\"\\n特征统计描述:\")\n",
    "print(all_points[features].describe())\n",
    "\n",
    "# 假设 all_points 是你的数据集\n",
    "missing_values = all_points[features].isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_data(all_points, features):\n",
    "    \"\"\"\n",
    "    处理缺失数据（修复数据类型问题）\n",
    "    \"\"\"\n",
    "    print(\"\\n开始处理缺失数据...\")\n",
    "    data = all_points.copy()\n",
    "    \n",
    "    # 确保数值型列的类型为float32\n",
    "    numerical_features = [f for f in features if f != 'clcd']\n",
    "    for feature in numerical_features:\n",
    "        data[feature] = data[feature].astype('float32')\n",
    "    \n",
    "    # 处理缺失值\n",
    "    for feature in features:\n",
    "        missing_count = data[feature].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"\\n处理 {feature} 的缺失值 ({missing_count} 个)...\")\n",
    "            \n",
    "            if feature == 'clcd':\n",
    "                # 使用loc替代fillna的inplace操作\n",
    "                mode_value = data[feature].mode()[0]\n",
    "                data.loc[data[feature].isnull(), feature] = mode_value\n",
    "                \n",
    "            else:\n",
    "                missing_ratio = missing_count / len(data)\n",
    "                \n",
    "                if missing_ratio < 0.05:\n",
    "                    # 使用相邻点的平均值填充\n",
    "                    mask = data[feature].isnull()\n",
    "                    coords = np.array([(p.x, p.y) for p in data.geometry])\n",
    "                    \n",
    "                    for idx in data[mask].index:\n",
    "                        current_point = coords[idx]\n",
    "                        distances = np.sqrt(np.sum((coords - current_point)**2, axis=1))\n",
    "                        \n",
    "                        valid_mask = ~data[feature].isnull()\n",
    "                        valid_distances = distances[valid_mask]\n",
    "                        valid_values = data[feature][valid_mask].values\n",
    "                        \n",
    "                        k = min(5, len(valid_values))\n",
    "                        nearest_indices = np.argpartition(valid_distances, k)[:k]\n",
    "                        \n",
    "                        weights = 1 / (valid_distances[nearest_indices] + 1e-6)\n",
    "                        weighted_mean = np.average(\n",
    "                            valid_values[nearest_indices], \n",
    "                            weights=weights\n",
    "                        ).astype('float32')\n",
    "                        \n",
    "                        data.loc[idx, feature] = weighted_mean\n",
    "                        \n",
    "                elif missing_ratio < 0.2:\n",
    "                    # 使用特征相关性填充\n",
    "                    corr = data[features].corr()[feature].abs()\n",
    "                    best_feature = corr.sort_values(ascending=False).index[1]\n",
    "                    \n",
    "                    train_mask = ~data[feature].isnull()\n",
    "                    X_train = data[best_feature][train_mask].values.reshape(-1, 1)\n",
    "                    y_train = data[feature][train_mask].values\n",
    "                    \n",
    "                    lr = LinearRegression()\n",
    "                    lr.fit(X_train, y_train)\n",
    "                    \n",
    "                    X_pred = data[best_feature][~train_mask].values.reshape(-1, 1)\n",
    "                    predicted_values = lr.predict(X_pred).astype('float32')\n",
    "                    data.loc[~train_mask, feature] = predicted_values\n",
    "                    \n",
    "                else:\n",
    "                    # 使用中位数填充\n",
    "                    median_value = data[feature].median()\n",
    "                    data.loc[data[feature].isnull(), feature] = median_value\n",
    "    \n",
    "    # 验证是否还有缺失值\n",
    "    remaining_missing = data[features].isnull().sum()\n",
    "    if remaining_missing.any():\n",
    "        print(\"\\n警告：仍存在缺失值:\")\n",
    "        print(remaining_missing[remaining_missing > 0])\n",
    "    else:\n",
    "        print(\"\\n所有缺失值已处理完成\")\n",
    "    \n",
    "    return data\n",
    "try:\n",
    "    all_points = handle_missing_data(all_points, features)\n",
    "except Exception as e:\n",
    "    print(f\"处理过程出错: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_processed_data(data, features, original_data=None):\n",
    "    \"\"\"\n",
    "    验证处理后的数据质量\n",
    "    \n",
    "    参数:\n",
    "        data: 处理后的DataFrame\n",
    "        features: 特征列表\n",
    "        original_data: 原始DataFrame（可选）\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 数据处理后验证 ===\")\n",
    "    \n",
    "    # 1. 基本检查\n",
    "    print(\"\\n1. 基本数据检查:\")\n",
    "    print(f\"样本数量: {len(data)}\")\n",
    "    print(f\"特征数量: {len(features)}\")\n",
    "    \n",
    "    # 2. 检查缺失值\n",
    "    missing = data[features].isnull().sum()\n",
    "    if missing.any():\n",
    "        print(\"\\n2. 发现缺失值:\")\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(\"\\n2. 无缺失值 ✓\")\n",
    "    \n",
    "    # 3. 检查数值范围\n",
    "    print(\"\\n3. 数值范围检查:\")\n",
    "    for feature in features:\n",
    "        if feature != 'clcd':  # 排除类别型数据\n",
    "            values = data[feature]\n",
    "            print(f\"\\n{feature}:\")\n",
    "            print(f\"  范围: [{values.min():.2f}, {values.max():.2f}]\")\n",
    "            print(f\"  均值: {values.mean():.2f}\")\n",
    "            print(f\"  标准差: {values.std():.2f}\")\n",
    "            \n",
    "            # 检查异常值\n",
    "            q1 = values.quantile(0.25)\n",
    "            q3 = values.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            outliers = values[(values < (q1 - 1.5 * iqr)) | (values > (q3 + 1.5 * iqr))]\n",
    "            if len(outliers) > 0:\n",
    "                print(f\"  异常值数量: {len(outliers)} ({(len(outliers)/len(values)*100):.2f}%)\")\n",
    "    \n",
    "    # 4. 检查类别分布\n",
    "    if 'clcd' in features:\n",
    "        print(\"\\n4. CLCD类别分布:\")\n",
    "        print(data['clcd'].value_counts())\n",
    "    \n",
    "    # 5. 如果有原始数据，比较处理前后的变化\n",
    "    if original_data is not None:\n",
    "        print(\"\\n5. 处理前后对比:\")\n",
    "        for feature in features:\n",
    "            if feature != 'clcd':\n",
    "                orig_mean = original_data[feature].mean()\n",
    "                proc_mean = data[feature].mean()\n",
    "                change_pct = ((proc_mean - orig_mean) / orig_mean * 100) if orig_mean != 0 else 0\n",
    "                print(f\"\\n{feature}:\")\n",
    "                print(f\"  原始均值: {orig_mean:.2f}\")\n",
    "                print(f\"  处理后均值: {proc_mean:.2f}\")\n",
    "                print(f\"  变化百分比: {change_pct:.2f}%\")\n",
    "    \n",
    "    # 6. 特征相关性检查\n",
    "    print(\"\\n6. 特征相关性检查:\")\n",
    "    corr_matrix = data[features].corr()\n",
    "    high_corr = np.where(np.abs(corr_matrix) > 0.8)\n",
    "    high_corr = [(features[i], features[j], corr_matrix.iloc[i, j]) \n",
    "                 for i, j in zip(*high_corr) if i < j]\n",
    "    if high_corr:\n",
    "        print(\"\\n发现高相关特征:\")\n",
    "        for f1, f2, corr in high_corr:\n",
    "            print(f\"{f1} - {f2}: {corr:.2f}\")\n",
    "    \n",
    "    # # 7. 可视化处理后的分布\n",
    "    #  plt.figure(figsize=(15, 5))\n",
    "    # for i, feature in enumerate(features[:3]):  # 展示前三个特征作为示例\n",
    "    #     if feature != 'clcd':\n",
    "    #         plt.subplot(1, 3, i+1)\n",
    "    #         sns.histplot(data=data, x=feature, hue='label', bins=30)\n",
    "    #         plt.title(f'{feature} 分布')\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "# 在主处理流程中使用\n",
    "try:\n",
    "    # 保存原始数据的副本\n",
    "    original_data = all_points.copy()\n",
    "    \n",
    "    # 处理缺失数据\n",
    "    all_points = handle_missing_data(all_points, features)\n",
    "    \n",
    "    # 验证处理后的数据\n",
    "    validate_processed_data(all_points, features, original_data)\n",
    "    \n",
    "    # ... 继续后续处理 ...\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"处理过程出错: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 特征重要性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(all_points, features):\n",
    "    \"\"\"\n",
    "    数据预处理：包括分类变量编码、数值标准化和空间特征提取\n",
    "    \"\"\"\n",
    "    print(\"\\n开始数据预处理...\")\n",
    "    \n",
    "    # 1. 分离类别型和数值型特征\n",
    "    categorical_features = ['clcd']\n",
    "    numerical_features = [f for f in features if f not in categorical_features]\n",
    "    \n",
    "    # 2. 处理类别型特征\n",
    "    print(\"\\n处理类别型特征...\")\n",
    "    enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    categorical_data = all_points[categorical_features].values\n",
    "    categorical_encoded = enc.fit_transform(categorical_data)\n",
    "    \n",
    "    # 获取编码后的特征名称\n",
    "    categorical_names = [f'clcd_{i+1}' for i in range(categorical_encoded.shape[1])]\n",
    "    \n",
    "    # 3. 处理数值型特征\n",
    "    print(\"\\n处理数值型特征...\")\n",
    "    numerical_data = all_points[numerical_features].values.astype(np.float32)\n",
    "    \n",
    "    # 4. 提取空间特征\n",
    "    print(\"\\n提取空间特征...\")\n",
    "    coords = np.column_stack([\n",
    "        all_points.geometry.x,\n",
    "        all_points.geometry.y\n",
    "    ])\n",
    "    \n",
    "    # 5. 标准化数值特征和空间特征\n",
    "    print(\"\\n特征标准化...\")\n",
    "    scaler = StandardScaler()\n",
    "    numerical_scaled = scaler.fit_transform(numerical_data)\n",
    "    coords_scaled = scaler.fit_transform(coords)\n",
    "    \n",
    "    # 6. 合并所有特征\n",
    "    X = np.column_stack([\n",
    "        numerical_scaled,    # 标准化的数值特征\n",
    "        categorical_encoded, # 独热编码的类别特征\n",
    "        coords_scaled       # 标准化的空间特征\n",
    "    ])\n",
    "    \n",
    "    # 7. 准备特征名称列表\n",
    "    feature_names = (\n",
    "        numerical_features +    # 数值特征名称\n",
    "        categorical_names +     # 类别特征编码后的名称\n",
    "        ['x_coord', 'y_coord'] # 空间特征名称\n",
    "    )\n",
    "    \n",
    "    # 8. 准备标签\n",
    "    y = all_points['label'].values\n",
    "    \n",
    "    # 9. 创建处理后的DataFrame\n",
    "    processed_df = pd.DataFrame(X, columns=feature_names)\n",
    "    processed_df['label'] = y\n",
    "    \n",
    "    print(\"\\n数据预处理完成!\")\n",
    "    print(f\"处理后特征数量: {len(feature_names)}\")\n",
    "    print(f\"样本数量: {len(processed_df)}\")\n",
    "    \n",
    "    return processed_df, X, y, scaler, enc\n",
    "def validate_preprocessing(processed_df, original_df, feature_names):\n",
    "    \"\"\"\n",
    "    验证预处理结果\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 预处理验证 ===\")\n",
    "    \n",
    "    # 1. 检查缺失值\n",
    "    missing = processed_df.isnull().sum()\n",
    "    if missing.any():\n",
    "        print(\"\\n警告: 存在缺失值:\")\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(\"\\n✓ 无缺失值\")\n",
    "    \n",
    "    # 2. 检查数值范围\n",
    "    print(\"\\n数值特征统计:\")\n",
    "    print(processed_df.describe())\n",
    "    \n",
    "    # 3. 检查类别编码\n",
    "    if 'clcd' in original_df.columns:\n",
    "        print(\"\\n类别编码验证:\")\n",
    "        print(\"原始CLCD类别:\", original_df['clcd'].unique())\n",
    "        clcd_cols = [col for col in processed_df.columns if col.startswith('clcd_')]\n",
    "        print(\"编码后的CLCD列:\", clcd_cols)\n",
    "    \n",
    "    # 4. 检查空间特征\n",
    "    print(\"\\n空间特征范围:\")\n",
    "    print(processed_df[['x_coord', 'y_coord']].describe())\n",
    "# 使用示例\n",
    "try:\n",
    "    # 假设 all_points 是你的数据集\n",
    "    # 定义特征列表\n",
    "    features = ['dem', 'slope', 'aspect', 'curvature', 'profile_curvature', \n",
    "               'plan_curvature', 'twi', 'spi', 'tpi', 'tri', 'ap', 'amdp', \n",
    "               'ahrf', 'clcd', 'ndvi', 'd2r', 'dd']\n",
    "    \n",
    "    # 数据预处理\n",
    "    processed_df, X, y, scaler, encoder = preprocess_data(all_points, features)\n",
    "    \n",
    "    # 验证预处理结果\n",
    "    validate_preprocessing(processed_df, all_points, features)\n",
    "    \n",
    "    print(\"\\n预处理完成，可以进行GeoXGBoost模型训练\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"预处理过程出错: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 生成易感性地图"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geomap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
